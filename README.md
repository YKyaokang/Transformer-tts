## 学习并实践了这个项目

传统的ai 类似于我第一个项目 调用deepseek接口、工作流接口等 使用axios等请求API就能做

但是在一些场景 需要实时通信 咱们能否设想将大模型下载到本地中呢？  

这个开源项目就帮我们完成了我们的设想

但是呢 由于这个技术的迭代并没有那么快 所以导致其性能并不是如传统那般出色 特别是文本转语音的大模型技术 看似预期的效果不太好

但是，我相信在未来，随着技术的不断发展更新，这个技术会越来越优秀，让我们真正可以实现在浏览器本地 调用AI大模型


## 实际实现--端模型

1.将transformer的模型库下载到本地：当然这里需要考虑一些性能问题和用户体验问题，在性能问题方面：使用了单例模式；在用户体验方面：实时更新进度条Progress的组件

2.webworker：这点太重要了，可以说是项目的一个重难点，单独拿出来说，咱们使用webworker来开启多线程的下载，由于这个模型库的下载需要一定的时间，所以这是必要的！但是
上面说了咱们有单例模式进行优化，所以我们可以在此基础上减少一些性能消耗，确保每次项目运行时只调用一次，并且还是懒加载

3.利用上述下载的模型库中的多个模型进行配合：具体表现为：  
    利用单例模式下载这些 并实时返回给App线程的响应式数据
    env, // 配置AI模型运行环境
    Tensor, // AI 模型处理数据的基本单位
    AutoTokenizer, // AI 自行分词器
    SpeechT5ForTextToSpeech, // 文本转语音模型 语音的特征
    SpeechT5HifiGan // 语音合成模型 和音色合成

    通过单例模式中第一次下载（这里涉及到大量的Promise的异步优化，待会在下文会提到）：得到三个实例

    这三个实例tokenizer,model,vocoder的作用分别如下：
    - tokenizer：分词器，将我们的自然语言（I love Hugging face！）转化成token，利于后续的调用
    - model：核心操作的api，它的方法generate_speech可以返回一个waveform声波，这个返回结果奠定了我们后续对Blob、URL转换等处理的基础
    - vocoder:generate_speech里面的核心参数 表示合成器
4. 到这里咱们已经拿到了waveform的数据，后续就更涉及到一些数据格式转换的工作了，与AI大模型的相关性就并不是很强了

整个过程如下：
 文本输入 → 分词 → 文本特征
                     ↓
 说话人ID → 下载特征 → 张量 → 语音模型 → 语音特征 → 语音合成 → 最终音频

## 性能优化
1. 首先，使用了webworker，上面其实已经详细提到了，它帮我们完成“并发”下载，使得能够开启两个线程，同时进行任务
2. 使用了单例模式这种设计模式，若没有这种模式的话，在项目运行时，每次都要重新下载，使得代码的健壮性非常差


## 用户体验
将用户体验做好：
用户在客户端，webworker线程下载的同时，能够通过Progress的组件实时看到下载的进度
当下载完成后，会通过设置响应式状态setDisabled等方法 使得用户能够获取到信息：当前已经下载（加载完毕），接下来就是在本地大模型跑啦


    